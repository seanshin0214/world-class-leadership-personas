# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ë‹¤ë¥¸ ì‚¬ëŒì´ ì²˜ìŒ ì‚¬ìš©í•  ë•Œ

### 1. ì €ì¥ì†Œ í´ë¡  (ì§€ì‹ ë² ì´ìŠ¤ ìë™ ë‹¤ìš´ë¡œë“œ)

```bash
git clone https://github.com/seanshin0214/world-class-leadership-personas.git
cd world-class-leadership-personas
```

**ê²°ê³¼**: 
- âœ… í˜ë¥´ì†Œë‚˜ ë©”íƒ€ë°ì´í„° (community/)
- âœ… MCP ì„œë²„ ì½”ë“œ (src/)
- âœ… **ì§€ì‹ ë² ì´ìŠ¤ (knowledge-base/)** â† ìë™ í¬í•¨!

### 2. ì§€ì‹ ë² ì´ìŠ¤ í™•ì¸

```bash
# í˜„ì¬ í¬í•¨ëœ ì§€ì‹ ë² ì´ìŠ¤
ls knowledge-base/

# ì¶œë ¥:
410-llm-engineer/
  â””â”€â”€ core-competencies/
      â””â”€â”€ transformer-architectures.md (20 pages) âœ…
```

### 3. MCP ì„œë²„ ì‹¤í–‰

```bash
npm install
npm run dev
```

**ì´ì œ Claude Desktopì—ì„œ ì‚¬ìš© ê°€ëŠ¥!**

---

## ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ë°›ê¸°

### ë‹¤ë¥¸ ì‚¬ëŒì´ ìƒˆ ì§€ì‹ ì¶”ê°€í–ˆì„ ë•Œ

```bash
# ìµœì‹  ì§€ì‹ ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ
git pull origin main

# ì˜ˆì‹œ ì¶œë ¥:
Updating 40a527a..52e3007
Fast-forward
 knowledge-base/410-llm-engineer/core-competencies/prompt-engineering.md | 1200 ++++
 1 file changed, 1200 insertions(+)
```

**ìë™ìœ¼ë¡œ ë™ê¸°í™”ë¨!** âœ…

---

## ì§€ì‹ ë² ì´ìŠ¤ì— ê¸°ì—¬í•˜ê¸°

### ìƒˆ ë¬¸ì„œ ì¶”ê°€

```bash
# 1. ë¬¸ì„œ ì‘ì„±
mkdir -p knowledge-base/410-llm-engineer/case-studies
cat > knowledge-base/410-llm-engineer/case-studies/my-case-study.md << 'EOF'
# My LLM Case Study

[Write your content...]
EOF

# 2. Git ì¶”ê°€
git add knowledge-base/

# 3. ì»¤ë°‹
git commit -m "feat: Add my LLM case study"

# 4. í‘¸ì‹œ
git push origin main
```

### ê¸°ì¡´ ë¬¸ì„œ ìˆ˜ì •

```bash
# 1. ìˆ˜ì •
code knowledge-base/410-llm-engineer/core-competencies/transformer-architectures.md

# 2. ì»¤ë°‹
git add knowledge-base/
git commit -m "docs: Update transformer-architectures with new benchmarks"

# 3. í‘¸ì‹œ
git push origin main
```

---

## í˜„ì¬ ìƒíƒœ

### ì™„ì„±ëœ ì§€ì‹ ë² ì´ìŠ¤

```
âœ… 410-llm-engineer/
   â””â”€â”€ core-competencies/
       â””â”€â”€ transformer-architectures.md
           - Multi-Head Attention (ìˆ˜í•™ ì¦ëª…)
           - Positional Encodings (RoPE, ALiBi)
           - Flash Attention (ì½”ë“œ + ë²¤ì¹˜ë§ˆí¬)
           - ì‹¤ì œ ì‚¬ë¡€ (LLaMA-2-70B, GPT-4)
           - 20 pages, í”„ë¡œë•ì…˜ê¸‰ í’ˆì§ˆ
```

### ì‘ì—… ì¤‘

```
â³ 410-llm-engineer/ (ì§„í–‰ë¥ : 20%)
   â”œâ”€â”€ core-competencies/
   â”‚   â”œâ”€â”€ transformer-architectures.md âœ…
   â”‚   â””â”€â”€ prompt-engineering.md â³ (ë‹¤ìŒ ì‘ì—…)
   â”œâ”€â”€ case-studies/ â³
   â”œâ”€â”€ code-examples/ â³
   â””â”€â”€ research-papers/ â³
```

---

## RAG ì‘ë™ í…ŒìŠ¤íŠ¸

### ì‹œë‚˜ë¦¬ì˜¤

**ì§ˆë¬¸**: "LLaMA-2-70B ì¶”ë¡  ì†ë„ ìµœì í™” ë°©ë²•?"

**RAG ê²€ìƒ‰**:
1. Query ì„ë² ë”© ìƒì„±
2. knowledge-base/410-llm-engineer/ ê²€ìƒ‰
3. transformer-architectures.mdì—ì„œ ê´€ë ¨ ì„¹ì…˜ ë°œê²¬:
   - Flash Attention (3.8x speedup)
   - Multi-Query Attention (8x KV cache reduction)
   - Real benchmarks (A100 GPU)

**ì‘ë‹µ**:
```
Based on the knowledge base:

1. Use Flash Attention: 3.8x faster, reduces memory O(nÂ²) â†’ O(n)
   [Code example from transformer-architectures.md]

2. Apply Multi-Query Attention: 8x smaller KV cache
   [Specific implementation details]

3. Expected improvement: 9s â†’ 1.8s latency
   [Real benchmark data included]
```

**íš¨ê³¼**: ì¼ë°˜ì  ì¡°ì–¸ â†’ êµ¬ì²´ì , ì½”ë“œ í¬í•¨, ì¸¡ì • ê°€ëŠ¥í•œ ë‹µë³€

---

## ì›Œí¬í”Œë¡œìš° ìš”ì•½

```
ë¡œì»¬ ì‘ì„±
    â†“
git add knowledge-base/
    â†“
git commit -m "feat: ..."
    â†“
git push origin main
    â†“
ë‹¤ë¥¸ ì‚¬ìš©ì: git pull
    â†“
ìë™ ë™ê¸°í™” âœ…
```

---

## ë‹¤ìŒ ë‹¨ê³„

### ë‹¨ê¸° (1ì£¼ì¼)
- [ ] prompt-engineering.md ì‘ì„± (30 pages)
- [ ] case-studies/ ì¶”ê°€ (5ê°œ ì‚¬ë¡€)

### ì¤‘ê¸° (1ê°œì›”)
- [ ] 410-llm-engineer ì™„ì„± (100MB)
- [ ] 10ê°œ í•µì‹¬ í˜ë¥´ì†Œë‚˜ ì‹œì‘

### ì¥ê¸° (6ê°œì›”)
- [ ] 142ê°œ ì „ì²´ í˜ë¥´ì†Œë‚˜
- [ ] Git LFS ë§ˆì´ê·¸ë ˆì´ì…˜ (1GB ë„ë‹¬ ì‹œ)

---

**í˜„ì¬**: Gitì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ ì§ì ‘ ê´€ë¦¬ ì¤‘ âœ…  
**íš¨ê³¼**: Clone í•œ ë²ˆìœ¼ë¡œ ëª¨ë“  ì§€ì‹ ìë™ ë‹¤ìš´ë¡œë“œ  
**ì—…ë°ì´íŠ¸**: git pullë¡œ ìµœì‹  ì§€ì‹ ì¦‰ì‹œ ë°˜ì˜
